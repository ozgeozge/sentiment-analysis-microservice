{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Amazon Comprehend: Detect Sentiment service for scoring reviews\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/peckjon/hosting-ml-as-microservice/blob/master/part1/score_reviews_via_service.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain labelled reviews\n",
    "\n",
    "In order to test any of the sentiment analysis APIs, we need a labelled dataset of reviews and their sentiment polarity. We'll use NLTK to download the movie_reviews corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/ozge/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "\n",
    "download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "The files in movie_reviews have already been divided into two sets: positive ('pos') and negative ('neg'), so we can load the raw text of the reviews into two lists, one for each polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# extract words from reviews, pair with label\n",
    "\n",
    "reviews_pos = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    review = movie_reviews.raw(fileid)\n",
    "    reviews_pos.append(review)\n",
    "\n",
    "reviews_neg = []\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    review = movie_reviews.raw(fileid)\n",
    "    reviews_neg.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the scoring API\n",
    "\n",
    "Fill in this function with code that connects to the Amazon Comprehend API, and uses it to score a single review:\n",
    "\n",
    "* [Documentation - Amazon Comprehend: Detect Sentiment](https://docs.aws.amazon.com/comprehend/latest/dg/API_DetectSentiment.html)\n",
    "\n",
    "Your function must return either 'pos' or 'neg', so you'll need to make some decisions about how to map the results of the API call to one of these values. Amazon Comprehend can return \"NEUTRAL\" or \"MIXED\" for the Sentiment - if this happens, you will need to inspect the numeric values under the SentimentScore to see whether it leans toward positive or negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "client =  boto3.client('comprehend',region_name='eu-central-1',aws_access_key_id='',aws_secret_access_key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sentiment': 'NEGATIVE',\n",
       " 'SentimentScore': {'Positive': 0.00015661792713217437,\n",
       "  'Negative': 0.9997037053108215,\n",
       "  'Neutral': 0.00011433633335400373,\n",
       "  'Mixed': 2.5389967049704865e-05},\n",
       " }"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"My order was delayed by several days without any updates or communication from the seller. Terrible shipping service.\"\"\"\n",
    "\n",
    "client.detect_sentiment(Text=text, LanguageCode='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_review(review):\n",
    "    ret_dict=client.detect_sentiment(Text=review[:5000], LanguageCode='en')\n",
    "    \n",
    "    if (ret_dict['Sentiment']==\"NEGATIVE\"):\n",
    "        return 'neg'\n",
    "    elif (ret_dict['Sentiment']==\"POSITIVE\"):\n",
    "        return 'pos'\n",
    "    elif (ret_dict['SentimentScore']['Positive']> ret_dict['SentimentScore']['Negative']):\n",
    "        return 'pos'\n",
    "    else:\n",
    "        return 'neg'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score each review\n",
    "\n",
    "Now, we can use the function you defined to score each of the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 smaller subsets for testing\n",
    "subset_pos = reviews_pos[:10]\n",
    "subset_neg = reviews_neg[:10]\n",
    "\n",
    "results_pos = []\n",
    "# When comfortable with results switch `subset_pos` to reviews_pos`\n",
    "for review in subset_pos:\n",
    "    result = score_review(review)\n",
    "    results_pos.append(result)\n",
    "\n",
    "results_neg = []\n",
    "# When comfortable with results switch `subset_neg` to reviews_neg`\n",
    "for review in subset_neg:\n",
    "    result = score_review(review)\n",
    "    results_neg.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy\n",
    "\n",
    "For each of our known positive reviews, we can count the number which our function scored as 'pos', and use this to calculate the % accuracy. We repeaty this for negative reviews, and also for overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive reviews: 64.7% correct\n",
      "Negative reviews: 68.0% correct\n",
      "Overall accuracy: 66.35% correct\n"
     ]
    }
   ],
   "source": [
    "correct_pos = results_pos.count('pos')\n",
    "accuracy_pos = float(correct_pos) / len(results_pos)\n",
    "correct_neg = results_neg.count('neg')\n",
    "accuracy_neg = float(correct_neg) / len(results_neg)\n",
    "correct_all = correct_pos + correct_neg\n",
    "accuracy_all = float(correct_all) / (len(results_pos)+len(results_neg))\n",
    "\n",
    "print('Positive reviews: {}% correct'.format(accuracy_pos*100))\n",
    "print('Negative reviews: {}% correct'.format(accuracy_neg*100))\n",
    "print('Overall accuracy: {}% correct'.format(accuracy_all*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS: Use the entire review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are not happy with the results we are getting from Comprehend, instead of truncating the review, we can submit it part by part and combine the results at the end. This will be a bit more complex, but it _might_ give us a boost in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import boto3\n",
    "\n",
    "# You need to obtain valid credentials to call the Comprehend APIs\n",
    "#\n",
    "# You can find more information about the different types of credential\n",
    "# and how to obtain them on the following tutorial:\n",
    "# https://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html\n",
    "# \n",
    "# And you can learn how to configure credentials in the AWS Python SDK (boto3)\n",
    "# in its official documentation:\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n",
    "comprehend = boto3.client(\n",
    "    \"comprehend\",\n",
    "    aws_access_key_id=\"YOUR_AWS_ACCESS_KEY_ID_HERE\",\n",
    "    aws_secret_access_key=\"YOUR_AWS_SECRET_ACCESS_KEY_HERE\"\n",
    ")\n",
    "\n",
    "def score_review(review):   \n",
    "    # Comprehend has a limit of 5000 characters in the text we submit for sentiment\n",
    "    # detection. If the text is longer, we will need to submit it in parts and\n",
    "    # combine the results to obtain the overall score\n",
    "    if len(review) < 5000:\n",
    "        return score_short_review(review)\n",
    "    else:\n",
    "        return score_long_review(review)\n",
    "    \n",
    "\n",
    "def score_short_review(review):\n",
    "    response = comprehend.detect_sentiment(Text=review, LanguageCode=\"en\")\n",
    "    \n",
    "    detected_sentiment = response[\"Sentiment\"]\n",
    "    if detected_sentiment == \"POSITIVE\":\n",
    "        return 'pos'\n",
    "    elif detected_sentiment == \"NEGATIVE\":\n",
    "        return 'neg'\n",
    "    else:\n",
    "        # There isn't a clear sentiment, the result is NEUTRAL or MIXED\n",
    "        # We need to compare the scores to detect if either positive or negative\n",
    "        # is the dominant one\n",
    "        sentiment_score = response[\"SentimentScore\"]\n",
    "        if sentiment_score[\"Positive\"] >= sentiment_score[\"Negative\"]:\n",
    "            return 'pos'\n",
    "        else:\n",
    "            return 'neg'\n",
    "    \n",
    "def score_long_review(review):\n",
    "    max_supported_length = 5000\n",
    "    # We split the review in fragments of 5000 characters\n",
    "    review_fragments = [review[i:i+max_supported_length] for i in range(0, len(review), max_supported_length)]\n",
    "    \n",
    "    accumulated_scores = {\"Positive\": 0.0, \"Negative\": 0.0, \"Neutral\": 0.0, \"Mixed\": 0.0}\n",
    "    for fragment in review_fragments:\n",
    "        # We need to ensure that shorter fragments don't overcompensate over larger ones\n",
    "        # To do so, we'll use a coefficient representing the % of the max supported length\n",
    "        fragment_weight = len(fragment)/max_supported_length\n",
    "        \n",
    "        response = comprehend.detect_sentiment(Text=fragment, LanguageCode=\"en\")\n",
    "        sentiment_scores = response[\"SentimentScore\"]\n",
    "\n",
    "        for sentiment in sentiment_scores.keys():\n",
    "            accumulated_scores[sentiment] = accumulated_scores[sentiment] + sentiment_scores[sentiment] * fragment_weight\n",
    "        \n",
    "        return accumulated_scores\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
